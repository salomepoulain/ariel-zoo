{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc8f2cc",
   "metadata": {},
   "source": [
    "# Diversity Evolutionary Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from examples.z_ec_course.A3_template import NUM_OF_MODULES\n",
    "import numpy as np\n",
    "\n",
    "# from ariel_experiments.characterize.individual import analyze_neighbourhood\n",
    "# from ariel_experiments.characterize.population import (\n",
    "# get_full_analyzed_population,\n",
    "# matrix_derive_neighbourhood,\n",
    "# matrix_derive_neighbourhood_cross_pop,\n",
    "# )\n",
    "import torch\n",
    "import umap\n",
    "from rich.console import Console\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from ariel.body_phenotypes.robogen_lite.decoders.hi_prob_decoding import (\n",
    "    HighProbabilityDecoder,\n",
    ")\n",
    "from ariel.ec.a004 import (\n",
    "    EA,\n",
    "    EASettings,\n",
    "    EAStep,\n",
    "    Individual,\n",
    "    Population,\n",
    ")\n",
    "from ariel.ec.genotypes.nde.nde import NeuralDevelopmentalEncoding\n",
    "from ariel_experiments.characterize.canonical.core.toolkit import (\n",
    "    CanonicalToolKit as ctk,\n",
    ")\n",
    "\n",
    "console = Console()\n",
    "SEED = 42\n",
    "\n",
    "# Seed everything for determinism\n",
    "np.random.seed(SEED)\n",
    "RNG = np.random.default_rng(SEED)\n",
    "\n",
    "# Seed PyTorch for deterministic behavior\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9a3b4",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global var\n",
    "global_fitness_history = []\n",
    "global_best_fitness_history = []\n",
    "diversity_archive = []  # NEW: Archive of past tree hashes for novelty search\n",
    "\n",
    "# EA settings\n",
    "EA_CONFIG = EASettings(\n",
    "    is_maximisation=True,\n",
    "    num_of_generations=50,\n",
    "    target_population_size=100,\n",
    ")\n",
    "\n",
    "# evaluation settings\n",
    "SIM_CONFIG = ctk.SimilarityConfig()\n",
    "SIM_CONFIG.max_tree_radius = 10\n",
    "SIM_CONFIG.radius_strategy = ctk.CollectionStrategy.SUBTREES\n",
    "\n",
    "\n",
    "NUM_OF_MODULES = 20\n",
    "GENOTYPE_SIZE = 64\n",
    "SCALE = 8192  # ADDED: Following A3_template pattern\n",
    "\n",
    "MUTATION_RATE = 0.05  # FIXED: 5% mutation rate (was 0)\n",
    "\n",
    "# Selection pressure settings\n",
    "PARENT_TOURNAMENT_SIZE = 4\n",
    "SURVIVOR_TOURNAMENT_SIZE = 4\n",
    "\n",
    "# Archive settings\n",
    "MAX_ARCHIVE_SIZE = 500_000 \n",
    "\n",
    "GLOBAL_N_NEIGHBORS = EA_CONFIG.target_population_size - 1\n",
    "\n",
    "K_NEIGHBORS = 5\n",
    "\n",
    "# ADDED: Global NDE for deterministic genotype-to-phenotype mapping (following A3_template pattern)\n",
    "GLOBAL_NDE = NeuralDevelopmentalEncoding(number_of_modules=NUM_OF_MODULES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145df85",
   "metadata": {},
   "source": [
    "# Helper functions for EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8555bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "def record_mean_fitness(population: Population) -> Population:\n",
    "    # Only calculate mean for alive individuals\n",
    "    mean_fitness = np.mean([ind.fitness for ind in population if ind.alive])\n",
    "    global_fitness_history.append(mean_fitness)\n",
    "    return population\n",
    "\n",
    "\n",
    "def record_best_fitness(population: Population) -> Population:\n",
    "    if EA_CONFIG.is_maximisation:\n",
    "        best_fitness = np.max([ind.fitness for ind in population if ind.alive])\n",
    "    else:\n",
    "        best_fitness = np.min([ind.fitness for ind in population if ind.alive])\n",
    "\n",
    "    global_best_fitness_history.append(best_fitness)\n",
    "    return population\n",
    "\n",
    "\n",
    "def add_survivors_to_archive(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    Add only surviving individuals to the diversity archive.\n",
    "    This should be called AFTER survivor selection.\n",
    "    \"\"\"\n",
    "    global diversity_archive\n",
    "\n",
    "    # Get only alive individuals\n",
    "    survivors = [ind for ind in population if ind.alive]\n",
    "\n",
    "    # Collect their subtrees\n",
    "    survivor_subtrees = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_nde_genotype(ind.genotype_),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in survivors\n",
    "    ]\n",
    "\n",
    "    # Add to archive\n",
    "    diversity_archive.extend(survivor_subtrees)\n",
    "\n",
    "    # Limit archive size (keep most recent)\n",
    "    if len(diversity_archive) > MAX_ARCHIVE_SIZE:\n",
    "        console.print(f\"Archive max size reached at generation {len(global_fitness_history)}\")\n",
    "        diversity_archive = diversity_archive[-MAX_ARCHIVE_SIZE:]\n",
    "\n",
    "    console.print(f\"Added {len(survivor_subtrees)} survivors to archive. Total archive size: {len(diversity_archive)}\")\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c773de5",
   "metadata": {},
   "source": [
    "# EA functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babedbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_genotype_to_string(genotype: list) -> str:\n",
    "    \"\"\"Helper: decode genotype to phenotype string using GLOBAL_NDE.\"\"\"\n",
    "    matrixes = GLOBAL_NDE.forward(np.array(genotype))\n",
    "    hpd = HighProbabilityDecoder(num_modules=NUM_OF_MODULES)\n",
    "    graph = hpd.probability_matrices_to_graph(matrixes[0], matrixes[1], matrixes[2])\n",
    "    tree = ctk.from_graph(graph)\n",
    "    return ctk.to_string(tree)\n",
    "\n",
    "\n",
    "def float_creep(\n",
    "    individual: list[list[float]] | list[list[list[float]]],\n",
    "    mutation_probability: float,\n",
    "    mutation_step_size: float = 100.0,  # Small step relative to SCALE\n",
    ") -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Creep mutation: adds small random perturbations to genes.\n",
    "    \n",
    "    Args:\n",
    "        individual: Genotype to mutate\n",
    "        mutation_probability: Probability each gene mutates (0-1)\n",
    "        mutation_step_size: Maximum step size for mutation (default 100)\n",
    "    \"\"\"\n",
    "    ind_arr = np.array(individual)\n",
    "    shape = ind_arr.shape\n",
    "\n",
    "    # Generate SMALL mutation values (not full range!)\n",
    "    mutator = RNG.uniform(-mutation_step_size, mutation_step_size, size=shape)\n",
    "\n",
    "    # Determine which positions to mutate\n",
    "    do_mask = RNG.choice(\n",
    "        [1, 0],\n",
    "        size=shape,\n",
    "        p=[mutation_probability, 1 - mutation_probability],\n",
    "    )\n",
    "    \n",
    "    mutation_mask = mutator * do_mask\n",
    "    new_genotype = ind_arr + mutation_mask\n",
    "    return new_genotype.tolist()\n",
    "\n",
    "\n",
    "def make_random_robot(genotype_size: int = GENOTYPE_SIZE) -> Individual:\n",
    "    \"\"\"\n",
    "    Produces a robot with only its genotype.\n",
    "    Following A3_template.py pattern: uses uniform distribution from -SCALE to +SCALE.\n",
    "    \"\"\"\n",
    "    ind = Individual()\n",
    "    ind.genotype = [\n",
    "        RNG.uniform(-SCALE, SCALE, genotype_size).tolist(),\n",
    "        RNG.uniform(-SCALE, SCALE, genotype_size).tolist(),\n",
    "        RNG.uniform(-SCALE, SCALE, genotype_size).tolist(),\n",
    "    ]\n",
    "    ind.fitness = 0.0\n",
    "    ind.requires_eval = True\n",
    "    ind.tags['ctk_string'] = decode_genotype_to_string(ind.genotype)\n",
    "    return ind\n",
    "\n",
    "\n",
    "def parent_selection_tournament(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    K-tournament parent selection with configurable tournament size.\n",
    "    Higher tournament size = higher selection pressure.\n",
    "    \"\"\"\n",
    "    tournament_size = PARENT_TOURNAMENT_SIZE\n",
    "\n",
    "    # Run tournaments to select parents\n",
    "    num_parents = len(population) // 2  # Select ~50% as parents\n",
    "    selected_parents = []\n",
    "\n",
    "    for _ in range(num_parents):\n",
    "        # Randomly select tournament_size individuals\n",
    "        competitors = RNG.choice(population, size=min(tournament_size, len(population)), replace=False)\n",
    "\n",
    "        # Find the best in the tournament\n",
    "        if EA_CONFIG.is_maximisation:\n",
    "            winner = max(competitors, key=lambda ind: ind.fitness)\n",
    "        else:\n",
    "            winner = min(competitors, key=lambda ind: ind.fitness)\n",
    "\n",
    "        selected_parents.append(winner)\n",
    "\n",
    "    # Clear all ps tags first\n",
    "    for ind in population:\n",
    "        ind.tags[\"ps\"] = False\n",
    "\n",
    "    # Tag selected parents\n",
    "    for parent in selected_parents:\n",
    "        parent.tags[\"ps\"] = True\n",
    "\n",
    "    return population\n",
    "\n",
    "\n",
    "def crossover(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    Generational crossover with 100% reproduction rate.\n",
    "    Creates offspring equal to target population size by selecting parents with replacement.\n",
    "    \"\"\"\n",
    "    children = []\n",
    "\n",
    "    # Get selected parents (those with ps=True from parent selection)\n",
    "    parents = [ind for ind in population if ind.tags.get(\"ps\", False)]\n",
    "\n",
    "    # If no parents selected (shouldn't happen), use all population\n",
    "    if len(parents) == 0:\n",
    "        parents = population\n",
    "\n",
    "    # Create target_population_size offspring\n",
    "    for _ in range(EA_CONFIG.target_population_size):\n",
    "        child = Individual()\n",
    "\n",
    "        # Select two parents WITH replacement (can select same parent twice)\n",
    "        parent1 = RNG.choice(parents)\n",
    "        parent2 = RNG.choice(parents)\n",
    "\n",
    "        # Generate a NEW random mask for every child\n",
    "        shape = np.array(parent1.genotype).shape\n",
    "        mask = RNG.random(size=shape) < 0.5\n",
    "\n",
    "        # Perform uniform crossover\n",
    "        child.genotype = np.where(\n",
    "            mask,\n",
    "            np.array(parent1.genotype),\n",
    "            np.array(parent2.genotype),\n",
    "        ).tolist()\n",
    "\n",
    "        # Tags\n",
    "        child.requires_eval = True\n",
    "        child.tags[\"mut\"] = True\n",
    "        children.append(child)\n",
    "\n",
    "    population.extend(children)\n",
    "    return population\n",
    "\n",
    "\n",
    "def mutation(population: Population) -> Population:\n",
    "    \"\"\"Mutates offspring by adding small random noise to genotype values.\"\"\"\n",
    "    mutation_rate = MUTATION_RATE\n",
    "    \n",
    "    # Debug: only print first 3 individuals to avoid spam\n",
    "    # debug_count = 0\n",
    "    # max_debug = 3\n",
    "    \n",
    "    for ind in population:\n",
    "        if ind.tags.get(\"mut\", False):\n",
    "            # DEBUG: Show before mutation\n",
    "            # if debug_count < max_debug:\n",
    "                # print(f'\\n--- Individual {debug_count + 1} ---')\n",
    "            # print(f'Genotype BEFORE (first 5 genes): {ind.genotype[0][:5]}')\n",
    "            # print(decode_genotype_to_string(ind.genotype))\n",
    "            # print(decode_genotype_to_string(ind.genotype))\n",
    "            # print(decode_genotype_to_string(ind.genotype))\n",
    "            # print(decode_genotype_to_string(ind.genotype))\n",
    "            # print(decode_genotype_to_string(ind.genotype))\n",
    "\n",
    "            # print(f'Phenotype BEFORE: {before_string}')\n",
    "            \n",
    "            # Apply mutation\n",
    "            genes = ind.genotype\n",
    "            mutated = [\n",
    "                float_creep(individual=genes[0], mutation_probability=mutation_rate),\n",
    "                float_creep(individual=genes[1], mutation_probability=mutation_rate),\n",
    "                float_creep(individual=genes[2], mutation_probability=mutation_rate),\n",
    "            ]\n",
    "            ind.genotype = mutated\n",
    "            \n",
    "            # DEBUG: Show after mutation\n",
    "        # if debug_count < max_debug:\n",
    "            # print(f'Genotype AFTER (first 5 genes): {ind.genotype[0][:5]}')\n",
    "            # after_string = decode_genotype_to_string(ind.genotype)\n",
    "            # print(f'Phenotype AFTER: {after_string}')\n",
    "            \n",
    "            # if before_string == after_string:\n",
    "                # print('⚠️  WARNING: Phenotype unchanged!')\n",
    "            # else:\n",
    "                # print('✓ Phenotype changed')\n",
    "                \n",
    "                            \n",
    "            ind.tags[\"mut\"] = False\n",
    "            ind.requires_eval = True\n",
    "\n",
    "    return population\n",
    "\n",
    "\n",
    "def fitness_tester(population: Population) -> Population:\n",
    "    \"\"\"Simple fitness function for testing: length of tree string.\"\"\"\n",
    "    for ind in population:\n",
    "        if ind.requires_eval:\n",
    "            # Use global NDE + new HPD (following A3_template pattern)\n",
    "            matrixes = GLOBAL_NDE.forward(np.array(ind.genotype))\n",
    "            hpd = HighProbabilityDecoder(num_modules=NUM_OF_MODULES)\n",
    "            ind_graph = hpd.probability_matrices_to_graph(\n",
    "                matrixes[0], matrixes[1], matrixes[2],\n",
    "            )\n",
    "            ind.tags[\"ctk_string\"] = ctk.to_string(ctk.from_graph(ind_graph))\n",
    "            ind.fitness = len(ind.tags[\"ctk_string\"]) + 0.0\n",
    "            ind.requires_eval = False\n",
    "\n",
    "    return population\n",
    "\n",
    "\n",
    "def survivor_selection_tournament(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    K-tournament survivor selection with configurable tournament size.\n",
    "    Higher tournament size = higher selection pressure.\n",
    "    Repeatedly runs tournaments to eliminate worst individuals.\n",
    "    \"\"\"\n",
    "    tournament_size = SURVIVOR_TOURNAMENT_SIZE\n",
    "    current_pop_size = len([ind for ind in population if ind.alive])\n",
    "\n",
    "    while current_pop_size > EA_CONFIG.target_population_size:\n",
    "        # Get all alive individuals\n",
    "        alive = [ind for ind in population if ind.alive]\n",
    "\n",
    "        # Safety check\n",
    "        if len(alive) <= EA_CONFIG.target_population_size:\n",
    "            break\n",
    "\n",
    "        # Randomly select tournament_size individuals for tournament\n",
    "        competitors = RNG.choice(alive, size=min(tournament_size, len(alive)), replace=False)\n",
    "\n",
    "        # Find the WORST individual in the tournament (to eliminate)\n",
    "        if EA_CONFIG.is_maximisation:\n",
    "            # In maximization, lowest fitness is worst\n",
    "            loser = min(competitors, key=lambda ind: ind.fitness)\n",
    "        else:\n",
    "            # In minimization, highest fitness is worst\n",
    "            loser = max(competitors, key=lambda ind: ind.fitness)\n",
    "\n",
    "        # Eliminate the loser\n",
    "        loser.alive = False\n",
    "        current_pop_size -= 1\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bbea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_diversity_cum_cos(population: Population) -> Population:\n",
    "    # 1. Collect subtrees data\n",
    "    for ind in population:\n",
    "        if ind.requires_eval:\n",
    "            ind.tags['ctk_string'] = decode_genotype_to_string(ind.genotype)\n",
    "    \n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(ind.tags['ctk_string']),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "\n",
    "    n_pop = len(population)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius))\n",
    "\n",
    "    # Initialize the accumulator matrix (N x N)\n",
    "    cumulative_sim_matrix = np.zeros((n_pop, n_pop))\n",
    "\n",
    "    current_hasher = FeatureHasher(\n",
    "        n_features=2**20,\n",
    "        input_type=\"string\",\n",
    "    )\n",
    "\n",
    "    # 2. Iterate keys and accumulate similarity matrices\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "\n",
    "        # Transform is called repeatedly on the same object (Efficient)\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "\n",
    "        # Calculate Similarity\n",
    "        sim_matrix = cosine_similarity(count_matrix)\n",
    "\n",
    "        # Accumulate\n",
    "        cumulative_sim_matrix += sim_matrix\n",
    "\n",
    "    # 3. Average the matrix across all keys (radii)\n",
    "    final_sim_matrix = cumulative_sim_matrix / len(keys)\n",
    "    np.fill_diagonal(final_sim_matrix, 0)\n",
    "\n",
    "    row_sums = final_sim_matrix.sum(axis=1)\n",
    "    mean_similarity_to_others = row_sums / (n_pop - 1)\n",
    "    diversity_scores = 1.0 - mean_similarity_to_others\n",
    "\n",
    "    # 7. Assign to individuals\n",
    "    for i, ind in enumerate(population):\n",
    "        ind.fitness = float(diversity_scores[i])\n",
    "        ind.requires_eval = False\n",
    "        # nde = NeuralDevelopmentalEncoding(number_of_modules=NUM_OF_MODULES)\n",
    "        # hpd = HighProbabilityDecoder(num_modules=NUM_OF_MODULES)\n",
    "        # matrixes = nde.forward(np.array(ind.genotype))\n",
    "        # ind_graph = hpd.probability_matrices_to_graph(\n",
    "        #     matrixes[0], matrixes[1], matrixes[2],\n",
    "        # )\n",
    "        # ind.tags[\"ctk_string\"] = ctk.to_string(ctk.from_graph(ind_graph))\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_novelty_cum_cos(population: Population) -> Population:\n",
    "    global diversity_archive\n",
    "\n",
    "    # 1. Collect features and calculate sizes\n",
    "    current_subtrees = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(decode_genotype_to_string(ind.genotype)),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "\n",
    "    all_subtrees = diversity_archive + current_subtrees\n",
    "    n_total = len(all_subtrees)\n",
    "    n_pop = len(population)\n",
    "    n_archive = len(diversity_archive)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius))\n",
    "\n",
    "    # Handle initial empty state\n",
    "    if n_total == 0:\n",
    "        for ind in population:\n",
    "            ind.fitness = 0.5\n",
    "        diversity_archive.extend(current_subtrees)\n",
    "        return population\n",
    "\n",
    "    cumulative_sim_to_all = np.zeros(n_pop)\n",
    "\n",
    "    current_hasher = FeatureHasher(\n",
    "        n_features=2**20,\n",
    "        input_type=\"string\",\n",
    "    )\n",
    "\n",
    "    # --- Weighting Constants ---\n",
    "    # We want the archive comparison to dominate the novelty score.\n",
    "    # Current weight should be low (e.g., 5-10% max)\n",
    "    WEIGHT_CURRENT = 0.10  # Reduced to a much lower weight\n",
    "    WEIGHT_ARCHIVE = 1.0 - WEIGHT_CURRENT\n",
    "\n",
    "    # 3. Calculate similarity for each radius key\n",
    "    for key in keys:\n",
    "        all_corpus = [d.get(key, []) for d in all_subtrees]\n",
    "\n",
    "        # Transform all to raw counts (Sparse Matrix)\n",
    "        all_features = current_hasher.transform(all_corpus)\n",
    "\n",
    "        # CRITICAL FIX: Apply L2 Normalization (Required for Cosine Similarity)\n",
    "        all_features = normalize(all_features, norm=\"l2\", axis=1)\n",
    "\n",
    "        # Split into archive and current (NOW normalized)\n",
    "        if n_archive > 0:\n",
    "            archive_features = all_features[:n_archive]\n",
    "            current_features = all_features[n_archive:]\n",
    "\n",
    "            # --- Similarity to Archive (The primary driver) ---\n",
    "            sim_to_archive = cosine_similarity(current_features, archive_features)\n",
    "            mean_sim_to_archive = sim_to_archive.mean(axis=1)  # Average sim of current individual to the entire archive\n",
    "\n",
    "            # --- Similarity to Current (The low-weighted stabilizer) ---\n",
    "            sim_to_current = cosine_similarity(current_features)\n",
    "            np.fill_diagonal(sim_to_current, 0)  # Exclude self\n",
    "            mean_sim_to_current = sim_to_current.sum(axis=1) / (n_pop - 1) if n_pop > 1 else np.zeros(n_pop)\n",
    "\n",
    "            # Weighted average: Archive comparison dominates the score\n",
    "            mean_sim = (WEIGHT_ARCHIVE * mean_sim_to_archive) + (WEIGHT_CURRENT * mean_sim_to_current)\n",
    "        else:\n",
    "            # Handle first generation case (only current vs current)\n",
    "            current_features = all_features\n",
    "            sim_to_current = cosine_similarity(current_features)\n",
    "            np.fill_diagonal(sim_to_current, 0)\n",
    "            mean_sim = sim_to_current.sum(axis=1) / (n_pop - 1) if n_pop > 1 else np.zeros(n_pop)\n",
    "\n",
    "        cumulative_sim_to_all += mean_sim\n",
    "\n",
    "    # 4. Average across keys and convert to diversity\n",
    "    mean_similarity = cumulative_sim_to_all / len(keys)\n",
    "    diversity_scores = 1.0 - mean_similarity\n",
    "\n",
    "    # 5. Assign fitness\n",
    "    for i, ind in enumerate(population):\n",
    "        ind.fitness = float(diversity_scores[i])\n",
    "\n",
    "    # 6. Add current population to archive (Correctly done)\n",
    "    diversity_archive.extend(current_subtrees)\n",
    "\n",
    "    # 7. Limit archive size (Correctly done)\n",
    "    if len(diversity_archive) > MAX_ARCHIVE_SIZE:\n",
    "        console.print(f\"ARCHIVE MAX SIZE REACHED {len(global_fitness_history)}\")\n",
    "        diversity_archive = diversity_archive[-MAX_ARCHIVE_SIZE:]\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac317d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_diversity_cum_cos(population: Population) -> Population:\n",
    "    # 1. Collect subtrees data\n",
    "    for ind in population:\n",
    "        if ind.requires_eval:\n",
    "            ind.tags['ctk_string'] = decode_genotype_to_string(ind.genotype)\n",
    "    \n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(ind.tags['ctk_string']),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "\n",
    "    # console.print(subtrees_dicts)\n",
    "\n",
    "    n_pop = len(population)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "\n",
    "    # console.print(keys)\n",
    "\n",
    "    # Initialize the accumulator matrix (N x N)\n",
    "    cumulative_sim_matrix = np.zeros((n_pop, n_pop))\n",
    "\n",
    "    current_hasher = FeatureHasher(\n",
    "        n_features=2**20,\n",
    "        input_type=\"string\",\n",
    "    )\n",
    "\n",
    "    # 2. Iterate keys and accumulate similarity matrices\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "\n",
    "        # console.print(specific_corpus)\n",
    "\n",
    "        # Transform is called repeatedly on the same object (Efficient)\n",
    "        count_matrix = current_hasher.fit_transform(specific_corpus)\n",
    "\n",
    "        # console.print(count_matrix.toarray())\n",
    "\n",
    "        # Calculate Similarity\n",
    "        sim_matrix = cosine_similarity(count_matrix)\n",
    "\n",
    "        # console.print(sim_matrix)\n",
    "\n",
    "        # Accumulate\n",
    "        cumulative_sim_matrix += sim_matrix\n",
    "\n",
    "    # 3. Average the matrix across all keys (radii)\n",
    "    final_sim_matrix = cumulative_sim_matrix / len(keys)\n",
    "    np.fill_diagonal(final_sim_matrix, 0)\n",
    "\n",
    "    # console.print(final_sim_matrix)\n",
    "\n",
    "    row_sums = final_sim_matrix.sum(axis=1)\n",
    "\n",
    "    # console.print('row sums', row_sums)\n",
    "\n",
    "    mean_similarity_to_others = row_sums / (n_pop - 1)\n",
    "\n",
    "    # console.print('dev', (n_pop - 1))\n",
    "\n",
    "    # console.print('mean similarity to others', mean_similarity_to_others)\n",
    "\n",
    "    diversity_scores = 1.0 - mean_similarity_to_others\n",
    "\n",
    "    # console.print('diversity scores', diversity_scores)\n",
    "\n",
    "    # 7. Assign to individuals\n",
    "    for i, ind in enumerate(population):\n",
    "        ind.fitness = float(diversity_scores[i])\n",
    "        ind.requires_eval = False\n",
    "\n",
    "        # nde = NeuralDevelopmentalEncoding(number_of_modules=NUM_OF_MODULES)\n",
    "        # hpd = HighProbabilityDecoder(num_modules=NUM_OF_MODULES)\n",
    "        # matrixes = nde.forward(np.array(ind.genotype))\n",
    "        # ind_graph = hpd.probability_matrices_to_graph(\n",
    "        #     matrixes[0], matrixes[1], matrixes[2],\n",
    "        # )\n",
    "        # ind.tags[\"ctk_string\"] = ctk.to_string(ctk.from_graph(ind_graph))\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e79502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_diversity_tfidf_cos(population: Population) -> Population:\n",
    "    # 1. Collect subtrees data\n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(decode_genotype_to_string(ind.genotype)),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "\n",
    "    # console.print(subtrees_dicts)\n",
    "\n",
    "    n_pop = len(population)\n",
    "    # Corrected keys: range(max_tree_radius + 1)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "\n",
    "    # console.print(keys)\n",
    "\n",
    "    cumulative_sim_matrix = np.zeros((n_pop, n_pop))\n",
    "\n",
    "    # Initialize FeatureHasher and TF-IDF Transformer ONCE\n",
    "    current_hasher = FeatureHasher(\n",
    "        n_features=2**20,\n",
    "        input_type=\"string\",\n",
    "    )\n",
    "    # The TfidfTransformer will learn the IDF based on the current population's raw counts\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "\n",
    "    # 2. Iterate keys and accumulate similarity matrices\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "\n",
    "        # console.print(specific_corpus)\n",
    "\n",
    "        # --- Step A: Hash to get Raw Counts (CSR Matrix) ---\n",
    "        count_matrix = current_hasher.fit_transform(specific_corpus)\n",
    "\n",
    "        # --- Step B: Apply TF-IDF Transformation (Required .fit_transform) ---\n",
    "        # This converts raw counts into weighted features, penalizing common subtrees.\n",
    "        # This must be done inside the loop to calculate IDF based on the current population.\n",
    "        weighted_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
    "\n",
    "        # --- Step C: Apply L2 Normalization (Required for Cosine Similarity) ---\n",
    "        # The TfidfTransformer may include L2 norm if norm='l2' is set in its constructor.\n",
    "        # However, for safety and clarity when mixing tools, we explicitly normalize.\n",
    "        normalized_matrix = normalize(weighted_matrix, norm=\"l2\", axis=1)\n",
    "\n",
    "        # --- Step D: Calculate Similarity (on normalized vectors) ---\n",
    "        sim_matrix = cosine_similarity(normalized_matrix)  # Equivalent to normalized_matrix @ normalized_matrix.T\n",
    "\n",
    "        # console.print(sim_matrix)\n",
    "\n",
    "        # Accumulate\n",
    "        cumulative_sim_matrix += sim_matrix\n",
    "\n",
    "    # 3. Average the matrix across all keys (radii)\n",
    "    final_sim_matrix = cumulative_sim_matrix / len(keys)\n",
    "    np.fill_diagonal(final_sim_matrix, 0)\n",
    "\n",
    "    # console.print(final_sim_matrix)\n",
    "\n",
    "    row_sums = final_sim_matrix.sum(axis=1)\n",
    "    mean_similarity_to_others = row_sums / (n_pop - 1)\n",
    "    diversity_scores = 1.0 - mean_similarity_to_others\n",
    "\n",
    "    # 7. Assign to individuals\n",
    "    for i, ind in enumerate(population):\n",
    "        ind.fitness = float(diversity_scores[i])\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_diversity_cum_cos_archive(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    Archive-based novelty search: compares current individuals to archive.\n",
    "    Novelty = dissimilarity to archive (not to current population).\n",
    "    NOTE: Does NOT add to archive - that's handled by add_survivors_to_archive().\n",
    "    \"\"\"\n",
    "    global diversity_archive\n",
    "\n",
    "    # 1. Collect current population's subtrees\n",
    "    current_subtrees = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(decode_genotype_to_string(ind.genotype)),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "\n",
    "    n_pop = len(population)\n",
    "    n_archive = len(diversity_archive)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "\n",
    "    # Handle first generation (no archive yet)\n",
    "    if n_archive == 0:\n",
    "        console.print(\"First generation: no archive, assigning neutral fitness\")\n",
    "        for ind in population:\n",
    "            ind.fitness = 0.5  # Neutral novelty\n",
    "        return population\n",
    "\n",
    "    # Initialize similarity accumulator for each individual\n",
    "    cumulative_similarity_to_archive = np.zeros(n_pop)\n",
    "\n",
    "    current_hasher = FeatureHasher(\n",
    "        n_features=2**20,\n",
    "        input_type=\"string\",\n",
    "    )\n",
    "\n",
    "    # 2. For each tree radius, calculate similarity to archive\n",
    "    for key in keys:\n",
    "        # Get features for archive and current population\n",
    "        archive_corpus = [d.get(key, []) for d in diversity_archive]\n",
    "        current_corpus = [d.get(key, []) for d in current_subtrees]\n",
    "\n",
    "        # Transform archive and current separately\n",
    "        # NOTE: FeatureHasher doesn't need fitting, it's stateless\n",
    "        archive_features = current_hasher.transform(archive_corpus)\n",
    "        current_features = current_hasher.transform(current_corpus)\n",
    "\n",
    "        # Calculate similarity: current vs archive\n",
    "        # Result shape: (n_pop, n_archive)\n",
    "        # Each row = one current individual's similarity to all archive individuals\n",
    "        sim_to_archive = cosine_similarity(current_features, archive_features)\n",
    "\n",
    "        # For each current individual, take mean similarity to archive\n",
    "        mean_sim_per_individual = sim_to_archive.mean(axis=1)  # Shape: (n_pop,)\n",
    "\n",
    "        # Accumulate across tree radii\n",
    "        cumulative_similarity_to_archive += mean_sim_per_individual\n",
    "\n",
    "    # 3. Average across all tree radii\n",
    "    mean_similarity_to_archive = cumulative_similarity_to_archive / len(keys)\n",
    "\n",
    "    # 4. Convert similarity to novelty (dissimilarity)\n",
    "    novelty_scores = 1.0 - mean_similarity_to_archive\n",
    "\n",
    "    console.print(f\"Archive size: {n_archive}, Mean novelty: {novelty_scores.mean():.4f}\")\n",
    "\n",
    "    # 5. Assign fitness (higher novelty = higher fitness)\n",
    "    for i, ind in enumerate(population):\n",
    "        ind.fitness = float(novelty_scores[i])\n",
    "\n",
    "    # NOTE: Archive update removed - handled by add_survivors_to_archive()\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71004355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming K_NEIGHBORS is defined globally, e.g.,\n",
    "\n",
    "\n",
    "def fitness_cos_knn(population: Population) -> Population:\n",
    "    # --- Setup ---\n",
    "    K = K_NEIGHBORS  # Use the global K\n",
    "    \n",
    "    for ind in population:\n",
    "        if ind.requires_eval:\n",
    "            ind.tags['ctk_string'] = decode_genotype_to_string(ind.genotype)\n",
    "    \n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(ind.tags['ctk_string']),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "    \n",
    "    # subtrees_dicts = [\n",
    "    #     ctk.collect_tree_hash_config_mode(\n",
    "    #         ctk.from_string(decode_genotype_to_string(ind.genotype)),\n",
    "    #         config=SIM_CONFIG,\n",
    "    #     ) for ind in population\n",
    "    # ]\n",
    "\n",
    "    n_pop = len(population)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "    cumulative_sim_matrix = np.zeros((n_pop, n_pop))\n",
    "\n",
    "    current_hasher = FeatureHasher(n_features=2**20, input_type=\"string\")\n",
    "\n",
    "    # 1. Accumulate Similarity Matrix\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "\n",
    "        # Hash to get Raw Counts\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "\n",
    "        # Apply L2 Normalization (Required for Cosine Similarity)\n",
    "        normalized_matrix = normalize(count_matrix, norm=\"l2\", axis=1)\n",
    "\n",
    "        # Calculate Similarity\n",
    "        sim_matrix = cosine_similarity(normalized_matrix)\n",
    "\n",
    "        # Accumulate\n",
    "        cumulative_sim_matrix += sim_matrix\n",
    "\n",
    "    # 2. Final Averaging (Average similarity across all keys)\n",
    "    final_sim_matrix = cumulative_sim_matrix / len(keys)\n",
    "\n",
    "    # 3. CRITICAL STEP: Convert Similarity to Distance/Dissimilarity\n",
    "    # Distance = 1 - Similarity. This is the required input for kNN distance summation.\n",
    "    distance_matrix = 1.0 - final_sim_matrix\n",
    "\n",
    "    # 4. Calculate K-Nearest Neighbor Novelty Score (Raw Sum)\n",
    "\n",
    "    N = distance_matrix.shape[0]\n",
    "\n",
    "    if N <= K:\n",
    "        # If population size is too small, use all available neighbors (excluding self)\n",
    "        k_actual = N - 1\n",
    "    else:\n",
    "        k_actual = K\n",
    "\n",
    "    if k_actual == 0:\n",
    "        # Avoid division by zero/zero summation if population is size 1\n",
    "        novelty_scores_sum = np.zeros(N)\n",
    "    else:\n",
    "        # 4a. Find the k+1 smallest distances in each row (including self-distance=0)\n",
    "        # argpartition is used for efficiency.\n",
    "        k_plus_1_indices = np.argpartition(distance_matrix, k_actual + 1, axis=1)\n",
    "\n",
    "        # 4b. Select the indices of the actual k nearest neighbors (skipping index 0)\n",
    "        knn_indices = k_plus_1_indices[:, 1:k_actual + 1]\n",
    "\n",
    "        # 4c. Gather the actual distance values\n",
    "        knn_distances = np.take_along_axis(distance_matrix, knn_indices, axis=1)\n",
    "\n",
    "        # 4d. Collapse: Sum the k distances for each row (Raw Novelty Score)\n",
    "        novelty_scores_sum = knn_distances.sum(axis=1)\n",
    "\n",
    "    # 5. Assign to individuals (Raw sum is the fitness score)\n",
    "    for i, ind in enumerate(population):\n",
    "        # We maximize the raw sum of distances (higher sum = higher novelty/fitness)\n",
    "        ind.fitness = float(novelty_scores_sum[i])\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Note: You must ensure 'umap' is imported and accessible.\n",
    "\n",
    "# Assuming the global K_NEIGHBORS (for fitness) and GLOBAL_N_NEIGHBORS (for UMAP internal k) are defined.\n",
    "\n",
    "\n",
    "def evaluate_umap_knn_fitness(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    Calculates Novelty Fitness based on the sum of Euclidean distances to K nearest\n",
    "    neighbors in the UMAP-transformed space (Maximization).\n",
    "    \"\"\"\n",
    "    # Use global K values\n",
    "    K = K_NEIGHBORS\n",
    "    K_UMAP_INTERNAL = 2\n",
    "\n",
    "    # --- 1. Setup & Data Collection ---\n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(decode_genotype_to_string(ind.genotype)),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "\n",
    "    n_pop = len(population)\n",
    "    # Corrected keys: range(max_tree_radius + 1)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "\n",
    "    # Initialize accumulator for the distance matrix (not similarity)\n",
    "    # We will accumulate distance matrices, not similarity matrices.\n",
    "    cumulative_dist_matrix = np.zeros((n_pop, n_pop))\n",
    "\n",
    "    # Initialize shared transformers\n",
    "    current_hasher = FeatureHasher(n_features=2**20, input_type=\"string\")\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=True)  # Recommended for structure\n",
    "\n",
    "    # 2. Accumulate Distance Matrix (Iterate over radii)\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "\n",
    "        # --- A. Hashing & Weighting (Hash -> TF-IDF) ---\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "        # Using TF-IDF to focus on unique structure, penalizing common subtrees\n",
    "        weighted_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
    "\n",
    "        # --- B. Dimensionality Reduction (UMAP) ---\n",
    "        # UMAP transforms the high-dim weighted vector into a low-dim embedding\n",
    "        umap_model = umap.UMAP(\n",
    "            init=\"random\",\n",
    "            random_state=42,\n",
    "            transform_seed=42,\n",
    "            n_jobs=1,\n",
    "            metric=\"cosine\",  # Use cosine distance in the high-dim space for graph construction\n",
    "            n_neighbors=K_UMAP_INTERNAL,\n",
    "        )\n",
    "        umap_embeddings = umap_model.fit_transform(weighted_matrix)\n",
    "\n",
    "        # --- C. Distance Calculation (Euclidean in UMAP space) ---\n",
    "        # pdist generates condensed distance vector; squareform converts to N x N matrix.\n",
    "        condensed_distances = pdist(umap_embeddings, metric=\"euclidean\")\n",
    "        distance_matrix = squareform(condensed_distances)\n",
    "\n",
    "        # Accumulate the distance matrices\n",
    "        cumulative_dist_matrix += distance_matrix\n",
    "\n",
    "    # 3. Final Averaging (Average distance across all keys)\n",
    "    final_dist_matrix = cumulative_dist_matrix / len(keys)\n",
    "\n",
    "    # 4. Calculate K-Nearest Neighbor Novelty Score (Raw Sum)\n",
    "\n",
    "    N = final_dist_matrix.shape[0]\n",
    "\n",
    "    k_actual = N - 1 if N <= K else K\n",
    "\n",
    "    if k_actual == 0:\n",
    "        novelty_scores_sum = np.zeros(N)\n",
    "    else:\n",
    "        # 4a. Find the k+1 smallest distances in each row (including self-distance=0)\n",
    "        k_plus_1_indices = np.argpartition(final_dist_matrix, k_actual + 1, axis=1)\n",
    "\n",
    "        # 4b. Select the indices of the actual k nearest neighbors (skipping index 0)\n",
    "        knn_indices = k_plus_1_indices[:, 1:k_actual + 1]\n",
    "\n",
    "        # 4c. Gather the actual distance values\n",
    "        knn_distances = np.take_along_axis(final_dist_matrix, knn_indices, axis=1)\n",
    "\n",
    "        # 4d. Collapse: Sum the k distances for each row (Raw Novelty Score)\n",
    "        novelty_scores_sum = knn_distances.sum(axis=1)\n",
    "\n",
    "    # 5. Assign to individuals (Raw sum is the fitness score)\n",
    "    for i, ind in enumerate(population):\n",
    "        # Higher sum of distances means higher novelty/fitness\n",
    "        ind.fitness = float(novelty_scores_sum[i])\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a794a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack  # REQUIRED for horizontal stacking\n",
    "\n",
    "\n",
    "def umap_aggregate_single_pass(population: Population) -> Population:\n",
    "\n",
    "    # Use global K values\n",
    "    K = K_NEIGHBORS\n",
    "    K_UMAP_INTERNAL = 2\n",
    "\n",
    "    # --- 1. Data Collection & Hashing ---\n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(ctk.from_string(decode_genotype_to_string(ind.genotype)), config=SIM_CONFIG)\n",
    "        for ind in population\n",
    "    ]\n",
    "\n",
    "    len(population)\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "\n",
    "    # Initialize shared transformers\n",
    "    current_hasher = FeatureHasher(n_features=2**21, input_type=\"string\")\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "\n",
    "    # List to hold the weighted feature matrix for each radius (to be stacked)\n",
    "    weighted_feature_list = []\n",
    "\n",
    "    # 2. Consolidated Feature Extraction & Weighting\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "\n",
    "        # Hash to get Raw Counts\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "\n",
    "        # Apply TF-IDF (This step MUST be inside the loop to calculate IDF per key)\n",
    "        weighted_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
    "\n",
    "        weighted_feature_list.append(weighted_matrix)\n",
    "\n",
    "    # 3. Aggregate Features (Horizontal Stack)\n",
    "    # The final matrix now has shape (N_pop x Total_Features)\n",
    "    final_feature_matrix = hstack(weighted_feature_list)\n",
    "\n",
    "    # 4. Dimensionality Reduction (UMAP - Single Pass)\n",
    "    umap_model = umap.UMAP(\n",
    "        init=\"random\",\n",
    "        random_state=42,\n",
    "        transform_seed=42,\n",
    "        n_jobs=1,\n",
    "        metric=\"cosine\",  # Metric on the high-dimensional space\n",
    "        n_neighbors=K_UMAP_INTERNAL,\n",
    "    )\n",
    "    # Fit and transform the entire consolidated feature set\n",
    "    umap_embeddings = umap_model.fit_transform(final_feature_matrix)\n",
    "\n",
    "    # 5. Distance Calculation (Euclidean in UMAP space)\n",
    "    condensed_distances = pdist(umap_embeddings, metric=\"euclidean\")\n",
    "    distance_matrix = squareform(condensed_distances)\n",
    "\n",
    "    # 6. Calculate K-Nearest Neighbor Novelty Score (Raw Sum)\n",
    "\n",
    "    N = distance_matrix.shape[0]\n",
    "\n",
    "    k_actual = N - 1 if N <= K else K\n",
    "\n",
    "    if k_actual == 0:\n",
    "        novelty_scores_sum = np.zeros(N)\n",
    "    else:\n",
    "        k_plus_1_indices = np.argpartition(distance_matrix, k_actual + 1, axis=1)\n",
    "        knn_indices = k_plus_1_indices[:, 1:k_actual + 1]\n",
    "        knn_distances = np.take_along_axis(distance_matrix, knn_indices, axis=1)\n",
    "        novelty_scores_sum = knn_distances.sum(axis=1)\n",
    "\n",
    "    # 7. Assign to individuals\n",
    "    for i, ind in enumerate(population):\n",
    "        ind.fitness = float(novelty_scores_sum[i])\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22102807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz, vstack\n",
    "\n",
    "\n",
    "def save_generation_features(\n",
    "    sparse_matrix: csr_matrix,\n",
    "    generation_number: int,\n",
    "    # DEFAULT IS SET HERE\n",
    "    base_path: str = \"./__data__/archive/features\",\n",
    ") -> None:\n",
    "    \"\"\"Saves the sparse feature matrix for the current generation quickly.\"\"\"\n",
    "    # Ensure the directory exists before attempting to save\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "\n",
    "    filename = os.path.join(base_path, f\"features_g{generation_number:04d}.npz\")\n",
    "    save_npz(filename, sparse_matrix)\n",
    "\n",
    "    # Note: You would update the other function signatures (like load_and_stack_all_features)\n",
    "    # to also use this same default path.\n",
    "\n",
    "\n",
    "def load_and_stack_all_features(base_path: str) -> csr_matrix | None:\n",
    "    \"\"\"Loads all historical sparse feature files and stacks them into one matrix.\"\"\"\n",
    "    file_list = sorted(glob.glob(os.path.join(base_path, \"features_g*.npz\")))\n",
    "\n",
    "    if not file_list:\n",
    "        return None\n",
    "\n",
    "    all_matrices = [load_npz(f) for f in file_list]\n",
    "\n",
    "    # Vertically stack all generations into one massive sparse matrix\n",
    "    return vstack(all_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming current_gen_count is available (e.g., from a global tracker)\n",
    "def extract_and_save_current_features(population: Population, current_gen_count: int, base_path: str):\n",
    "    \"\"\"Hashes the current population's subtrees and saves the raw count matrix.\"\"\"\n",
    "    # 1. Collect features (similar to your previous setup)\n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(ctk.from_string(decode_genotype_to_string(ind.genotype)), config=SIM_CONFIG)\n",
    "        for ind in population\n",
    "    ]\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "\n",
    "    # Initialize Hasher (Only needs to happen once for the raw features)\n",
    "    current_hasher = FeatureHasher(n_features=2**20, input_type=\"string\")\n",
    "\n",
    "    # List to hold the raw feature matrix for each radius (to be stacked horizontally)\n",
    "    raw_feature_list = []\n",
    "\n",
    "    # 2. Consolidated Feature Extraction (No TF-IDF or L2 yet!)\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "        raw_feature_list.append(count_matrix)\n",
    "\n",
    "    # 3. Aggregate Features (Horizontal Stack)\n",
    "    raw_feature_matrix_gen = hstack(raw_feature_list)\n",
    "\n",
    "    # 4. Save the raw feature matrix for this generation\n",
    "    save_generation_features(raw_feature_matrix_gen, current_gen_count, base_path)\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3165e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "\n",
    "def fitness_rarity_magnitude(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    Calculates fitness based on the L2 Norm (Magnitude) of the aggregated,\n",
    "    TF-IDF weighted feature vector for each individual (Rarity Magnitude Search).\n",
    "    \"\"\"\n",
    "    # --- 1. Setup & Data Collection ---\n",
    "    subtrees_dicts = [\n",
    "        ctk.collect_tree_hash_config_mode(\n",
    "            ctk.from_string(decode_genotype_to_string(ind.genotype)),\n",
    "            config=SIM_CONFIG,\n",
    "        ) for ind in population\n",
    "    ]\n",
    "\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "\n",
    "    # Initialize shared transformers\n",
    "    current_hasher = FeatureHasher(n_features=2**20, input_type=\"string\")\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=True, norm=\"l2\")  # Ensure final L2 norm is applied\n",
    "\n",
    "    weighted_feature_list = []\n",
    "\n",
    "    # 2. Consolidated Feature Extraction & Weighting\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "\n",
    "        # Hash to get Raw Counts\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "\n",
    "        # Apply TF-IDF (This step calculates the final L2 norm on the feature vector)\n",
    "        # We use .fit_transform here to correctly learn the IDF based on the current population.\n",
    "        weighted_matrix = tfidf_transformer.fit_transform(count_matrix)\n",
    "\n",
    "        weighted_feature_list.append(weighted_matrix)\n",
    "\n",
    "    # 3. Aggregate Features (Horizontal Stack)\n",
    "    # The final matrix now has shape (N_pop x Total_Features)\n",
    "    final_feature_matrix = hstack(weighted_feature_list)\n",
    "\n",
    "    # 4. Calculate the Rarity Magnitude (L2 Norm)\n",
    "\n",
    "    # The L2 norm of a sparse matrix is calculated efficiently using .power(2) and .sum(axis=1)\n",
    "    if issparse(final_feature_matrix):\n",
    "        # Calculate the sum of squares for each row\n",
    "        # (This is equivalent to the square of the L2 norm)\n",
    "        square_norms = final_feature_matrix.power(2).sum(axis=1)\n",
    "\n",
    "        # Calculate the square root to get the final L2 norm\n",
    "        rarity_magnitude_scores = np.sqrt(square_norms).A.flatten()\n",
    "    else:\n",
    "        # Fallback for dense matrix (less likely here)\n",
    "        rarity_magnitude_scores = np.linalg.norm(final_feature_matrix, axis=1)\n",
    "\n",
    "    # 5. Assign to individuals (Fitness = Rarity Magnitude)\n",
    "    for i, ind in enumerate(population):\n",
    "        # Higher magnitude means higher rarity and complexity, maximizing the score.\n",
    "        ind.fitness = float(rarity_magnitude_scores[i])\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "# Global state variables\n",
    "GLOBAL_GENERATION_COUNT = 0\n",
    "GLOBAL_RAW_HISTORY = None\n",
    "GLOBAL_TFIDF = TfidfTransformer(use_idf=True)  # FIXED: Removed norm='l2' to avoid double normalization\n",
    "\n",
    "\n",
    "def clear_archive(base_path=\"./__data__/archive/features\") -> None:\n",
    "    \"\"\"Deletes the archive directory to start fresh.\"\"\"\n",
    "    if pathlib.Path(base_path).exists():\n",
    "        shutil.rmtree(base_path)\n",
    "        console.print(f\"[yellow]Cleared archive at: {base_path}[/yellow]\")\n",
    "    else:\n",
    "        console.print(f\"[dim]No archive found at: {base_path}[/dim]\")\n",
    "\n",
    "\n",
    "def fitness_online_tfidf_archive(population: Population) -> Population:\n",
    "    \"\"\"\n",
    "    Calculates fitness using incremental TF-IDF and updates the archive.\n",
    "    Manages generation counting internally.\n",
    "    Auto-clears archive on first generation (gen 0).\n",
    "    Uses GLOBAL_NDE for deterministic decoding (following A3_template pattern).\n",
    "    \"\"\"\n",
    "    # Bring in the globals to track state across generations\n",
    "    global GLOBAL_RAW_HISTORY, GLOBAL_TFIDF, GLOBAL_GENERATION_COUNT\n",
    "\n",
    "    # Define the path (hardcoded here or use a global config constant)\n",
    "    base_path = \"./__data__/archive/features\"\n",
    "\n",
    "    # Auto-clear archive on first generation to prevent contamination from previous runs\n",
    "    if GLOBAL_GENERATION_COUNT == 0:\n",
    "        clear_archive(base_path)\n",
    "\n",
    "    # --- 1. Extract Raw Features for Current Population ---\n",
    "    # FIXED: Changed genotype_ to genotype\n",
    "    # FIXED: Using GLOBAL_NDE + new HPD each time (A3_template pattern)\n",
    "    subtrees_dicts = []\n",
    "    for ind in population:\n",
    "        matrixes = GLOBAL_NDE.forward(np.array(ind.genotype))\n",
    "        hpd = HighProbabilityDecoder(num_modules=NUM_OF_MODULES)\n",
    "        ind_graph = hpd.probability_matrices_to_graph(matrixes[0], matrixes[1], matrixes[2])\n",
    "        tree = ctk.from_graph(ind_graph)\n",
    "        subtree_dict = ctk.collect_tree_hash_config_mode(tree, config=SIM_CONFIG)\n",
    "        subtrees_dicts.append(subtree_dict)\n",
    "\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "    current_hasher = FeatureHasher(n_features=2**20, input_type=\"string\")\n",
    "\n",
    "    raw_feature_list = []\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "        raw_feature_list.append(count_matrix)\n",
    "\n",
    "    # The raw counts for THIS generation\n",
    "    current_raw_matrix = hstack(raw_feature_list)\n",
    "\n",
    "    # --- 2. Combine History + Current for Fitting ---\n",
    "    if GLOBAL_RAW_HISTORY is not None:\n",
    "        # Stack history on top of current to create the full corpus\n",
    "        full_corpus_for_fitting = vstack([GLOBAL_RAW_HISTORY, current_raw_matrix])\n",
    "    else:\n",
    "        # First generation: History is just the current population\n",
    "        full_corpus_for_fitting = current_raw_matrix\n",
    "\n",
    "    # --- 3. Refit TF-IDF on the Full Corpus ---\n",
    "    # This learns the IDF weights based on EVERYTHING found so far.\n",
    "    GLOBAL_TFIDF.fit(full_corpus_for_fitting)\n",
    "\n",
    "    # --- 4. Transform the ACTUAL (Current) Matrix ---\n",
    "    # Apply the newly learned weights to the current population\n",
    "    current_weighted_matrix = GLOBAL_TFIDF.transform(current_raw_matrix)\n",
    "\n",
    "    # --- 5. Calculate Fitness (L2 Norm / Rarity Magnitude) ---\n",
    "    if issparse(current_weighted_matrix):\n",
    "        square_norms = current_weighted_matrix.power(2).sum(axis=1)\n",
    "        rarity_scores = np.sqrt(square_norms).A.flatten()\n",
    "    else:\n",
    "        rarity_scores = np.linalg.norm(current_weighted_matrix, axis=1)\n",
    "\n",
    "    for i, ind in enumerate(population):\n",
    "        ind.fitness = float(rarity_scores[i])\n",
    "\n",
    "    # --- 6. Update the Archive (Disk & Memory) ---\n",
    "\n",
    "    # A. Save to disk using the Global Generation Count\n",
    "    save_generation_features(current_raw_matrix, GLOBAL_GENERATION_COUNT, base_path)\n",
    "\n",
    "    # B. Update Memory\n",
    "    GLOBAL_RAW_HISTORY = full_corpus_for_fitting\n",
    "\n",
    "    # C. Increment Generation Count for next time\n",
    "    GLOBAL_GENERATION_COUNT += 1\n",
    "\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f91296",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_GENERATION_COUNT = 0\n",
    "def save_featues(population: Population):\n",
    "    global GLOBAL_GENERATION_COUNT \n",
    "    GLOBAL_GENERATION_COUNT += 1\n",
    "    base_path = \"./__data__/archive/features\"    \n",
    "    \n",
    "    subtrees_dicts = []\n",
    "    for ind in population:\n",
    "        matrixes = GLOBAL_NDE.forward(np.array(ind.genotype))\n",
    "        hpd = HighProbabilityDecoder(num_modules=NUM_OF_MODULES)\n",
    "        ind_graph = hpd.probability_matrices_to_graph(matrixes[0], matrixes[1], matrixes[2])\n",
    "        tree = ctk.from_graph(ind_graph)\n",
    "        subtree_dict = ctk.collect_tree_hash_config_mode(tree, config=SIM_CONFIG)\n",
    "        subtrees_dicts.append(subtree_dict)\n",
    "\n",
    "    keys = list(range(SIM_CONFIG.max_tree_radius + 1))\n",
    "    current_hasher = FeatureHasher(n_features=2**20, input_type=\"string\")\n",
    "\n",
    "    raw_feature_list = []\n",
    "    for key in keys:\n",
    "        specific_corpus = [d.get(key, []) for d in subtrees_dicts]\n",
    "        count_matrix = current_hasher.transform(specific_corpus)\n",
    "        raw_feature_list.append(count_matrix)\n",
    "\n",
    "    current_raw_matrix = hstack(raw_feature_list)    \n",
    "    save_generation_features(current_raw_matrix, GLOBAL_GENERATION_COUNT, base_path)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb84859",
   "metadata": {},
   "source": [
    "# run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_list = [\n",
    "    make_random_robot(GENOTYPE_SIZE) for _ in range(EA_CONFIG.target_population_size)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fec3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = [\n",
    "    EAStep(\"parent_selection\", parent_selection_tournament),  # K-tournament (size=3)\n",
    "    EAStep(\"crossover\", crossover),\n",
    "    EAStep(\"mutation\", mutation),\n",
    "\n",
    "    \n",
    "    # EAStep('diveristy', evaluate_diversity_cum_cos),\n",
    "    # EAStep(\"tfidf_archive\", fitness_online_tfidf_archive),\n",
    "    # EAStep('umap_agg', umap_aggregate_single_pass),\n",
    "    # EAStep('umap', evaluate_umap_knn_fitness),\n",
    "    # EAStep('fitness', fitness_tester),\n",
    "    EAStep(\"evaluation\", fitness_cos_knn),\n",
    "    # EAStep(\"evaluation\", evaluate_diversity_tfidf_cos),\n",
    "    # EAStep(\"evaluation\", evaluate_novelty_cum_cos),\n",
    "    EAStep(\"survivor_selection\", survivor_selection_tournament),  # K-tournament (size=3)\n",
    "    # EAStep(\"archive_survivors\", add_survivors_to_archive),  # NEW: Archive only survivors\n",
    "    \n",
    "    # EAStep('save_featues', save_featues),\n",
    "    EAStep(\"record_fitness\", record_mean_fitness),\n",
    "    EAStep(\"best_fitness\", record_best_fitness),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ea = EA(\n",
    "    population_list,\n",
    "    operations=ops,\n",
    "    num_of_generations=EA_CONFIG.num_of_generations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ea.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fitness_history() -> None:\n",
    "    if not global_fitness_history:\n",
    "        return\n",
    "\n",
    "    generations = range(1, len(global_fitness_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(\n",
    "        generations,\n",
    "        global_fitness_history,\n",
    "        # marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        color=\"blue\",\n",
    "        label=\"mean\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        generations,\n",
    "        global_best_fitness_history,\n",
    "        # marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        color=\"red\",\n",
    "        label=\"best\",\n",
    "    )\n",
    "\n",
    "    y_max1 = np.max(global_fitness_history)\n",
    "    y_max2 = np.max(global_best_fitness_history)\n",
    "\n",
    "    y_max = np.max([y_max1, y_max2])\n",
    "\n",
    "    # --- THE FIX ---\n",
    "    # This forces the Y-axis to start at 0.\n",
    "    # Since your data is normalized (0 to 1), setting the top to 1.1 is also usually good practice.\n",
    "    # plt.ylim(0, y_max * 1.2)\n",
    "    # ----------------\n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    # plt.title(\"\")\n",
    "    plt.xlabel(\"Generations\")\n",
    "    plt.ylabel(\"Fitness\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_fitness_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bokeh.models import ColorBar, ColumnDataSource, HoverTool\n",
    "from bokeh.palettes import Turbo256  # Good palettes for time depth\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.transform import linear_cmap\n",
    "\n",
    "\n",
    "def plot_archive_time_lapse(\n",
    "    umap_coordinates: np.ndarray,\n",
    "    generation_ids: np.ndarray,\n",
    "    images: list | None = None,\n",
    "    title: str = \"Evolutionary Time-Lapse\",\n",
    "    plot_width: int = 800,\n",
    "    plot_height: int = 600,\n",
    "    point_size: int = 6,\n",
    "    alpha: float = 0.6,\n",
    "    selected_generations: list | None = None,  # NEW: Filter specific generations\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the entire evolutionary history in a single UMAP space.\n",
    "    Color indicates Generation Number (Time).\n",
    "\n",
    "    Args:\n",
    "        umap_coordinates: (N_total, 2) array of UMAP embeddings for the archive.\n",
    "        generation_ids: (N_total,) array of generation numbers corresponding to each point.\n",
    "        images: Optional list of base64 images (len = N_total). If None, tooltips show only ID/Gen.\n",
    "        selected_generations: Optional list of generation numbers to visualize.\n",
    "                            If None, all generations are plotted.\n",
    "                            Examples: [0, 1, 2] or range(10, 20) or [0, 25, 49]\n",
    "    \"\"\"\n",
    "    # Filter data if specific generations are requested\n",
    "    if selected_generations is not None:\n",
    "        selected_generations = list(selected_generations)  # Convert range to list if needed\n",
    "        mask = np.isin(generation_ids, selected_generations)\n",
    "\n",
    "        umap_coordinates = umap_coordinates[mask]\n",
    "        generation_ids = generation_ids[mask]\n",
    "\n",
    "        if images is not None:\n",
    "            images = [img for i, img in enumerate(images) if mask[i]]\n",
    "\n",
    "        console.print(f\"[cyan]Filtered to {len(selected_generations)} generations: {sorted(selected_generations)}[/cyan]\")\n",
    "        console.print(f\"[cyan]Plotting {mask.sum()} individuals[/cyan]\")\n",
    "\n",
    "    n_total = len(generation_ids)\n",
    "\n",
    "    if n_total == 0:\n",
    "        return\n",
    "\n",
    "    # 1. Setup Color Mapper (Time = Color)\n",
    "    # We use linear_cmap to map the 'generation' column directly to colors\n",
    "    # Turbo256 is great because it has high contrast (Red=New, Blue=Old)\n",
    "    color_mapper = linear_cmap(\n",
    "        field_name=\"generation\",\n",
    "        palette=Turbo256,\n",
    "        low=min(generation_ids),\n",
    "        high=max(generation_ids),\n",
    "    )\n",
    "\n",
    "    # 2. Build DataFrame\n",
    "    data = {\n",
    "        \"x\": umap_coordinates[:, 0],\n",
    "        \"y\": umap_coordinates[:, 1],\n",
    "        \"generation\": generation_ids,\n",
    "        \"id\": np.arange(n_total),\n",
    "    }\n",
    "\n",
    "    # 3. Handle Optional Images\n",
    "    if images is not None:\n",
    "        if len(images) != n_total:\n",
    "            tooltip_html = \"\"\"\n",
    "                <div style=\"font-size:14px; font-weight: bold;\">Gen: @generation</div>\n",
    "                <div style=\"font-size:12px; color:#666;\">ID: @id</div>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            data[\"image\"] = images\n",
    "            tooltip_html = \"\"\"\n",
    "                <div><img src='@image' style='float:left; margin:5px; width:100px; height:auto;'/></div>\n",
    "                <div style=\"font-size:14px; font-weight: bold;\">Gen: @generation</div>\n",
    "                <div style=\"font-size:12px; color:#666;\">ID: @id</div>\n",
    "            \"\"\"\n",
    "    else:\n",
    "        tooltip_html = \"\"\"\n",
    "            <div style=\"font-size:14px; font-weight: bold;\">Gen: @generation</div>\n",
    "            <div style=\"font-size:12px; color:#666;\">ID: @id</div>\n",
    "        \"\"\"\n",
    "\n",
    "    source = ColumnDataSource(data)\n",
    "\n",
    "    # 4. Create Plot\n",
    "    p = figure(\n",
    "        title=title,\n",
    "        width=plot_width,\n",
    "        height=plot_height,\n",
    "        tools=\"pan,wheel_zoom,reset,save\",\n",
    "        toolbar_location=\"above\",\n",
    "        match_aspect=True,  # Keeps UMAP aspect ratio correct\n",
    "    )\n",
    "\n",
    "    # 5. Scatter Layer\n",
    "    # Sort by generation so new points (bright) are drawn ON TOP of old points\n",
    "    # Note: Bokeh doesn't have simple z-order sorting in CDS, so we usually rely on input order.\n",
    "    # If your archive is already sorted by time, this is automatic.\n",
    "\n",
    "    renderer = p.scatter(\n",
    "        \"x\", \"y\",\n",
    "        source=source,\n",
    "        size=point_size,\n",
    "        fill_color=color_mapper,  # The magic time coloring\n",
    "        line_color=None,         # No border makes clusters clearer\n",
    "        fill_alpha=alpha,\n",
    "    )\n",
    "\n",
    "    # 6. Add Color Bar (Legend)\n",
    "    color_bar = ColorBar(\n",
    "        color_mapper=color_mapper[\"transform\"],\n",
    "        width=8,\n",
    "        location=(0, 0),\n",
    "        title=\"Generation\",\n",
    "        title_text_font_size=\"10pt\",\n",
    "    )\n",
    "    p.add_layout(color_bar, \"right\")\n",
    "\n",
    "    # 7. Add Tooltip\n",
    "    hover = HoverTool(tooltips=tooltip_html, renderers=[renderer])\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    # Optional: Clean up axes\n",
    "    p.xaxis.visible = False\n",
    "    p.yaxis.visible = False\n",
    "    p.xgrid.visible = False\n",
    "    p.ygrid.visible = False\n",
    "    p.background_fill_color = \"#fafafa\"\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_archive_and_generate_ids(base_path=\"./__data__/archive/features\"):\n",
    "    \"\"\"\n",
    "    Loads all .npz files, stacks them, and generates the corresponding\n",
    "    generation_id array based on the actual file contents.\n",
    "    \"\"\"\n",
    "    # 1. Find and sort files (e.g., features_g0000.npz, features_g0001.npz)\n",
    "    file_list = sorted(glob.glob(os.path.join(base_path, \"features_g*.npz\")))\n",
    "\n",
    "    if not file_list:\n",
    "        msg = \"No feature files found!\"\n",
    "        raise FileNotFoundError(msg)\n",
    "\n",
    "    matrices = []\n",
    "    gen_ids_list = []\n",
    "\n",
    "    for filepath in file_list:\n",
    "        # Extract generation number from filename \"features_g0050.npz\"\n",
    "        # Split by '_g', take the last part, remove '.npz', convert to int\n",
    "        try:\n",
    "            filename = pathlib.Path(filepath).name\n",
    "            gen_num = int(filename.split(\"_g\")[1].split(\".\")[0])\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "        # Load the matrix\n",
    "        matrix = load_npz(filepath)\n",
    "        matrices.append(matrix)\n",
    "\n",
    "        # Create an ID array of the same length as this generation's population\n",
    "        # shape[0] is the number of individuals in that specific generation\n",
    "        current_ids = np.full(matrix.shape[0], gen_num)\n",
    "        gen_ids_list.append(current_ids)\n",
    "\n",
    "    # 2. Stack everything\n",
    "    F_total = vstack(matrices)\n",
    "    all_gen_ids = np.concatenate(gen_ids_list)\n",
    "\n",
    "    return F_total, all_gen_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import umap.plot\n",
    "from bokeh.io import output_notebook\n",
    "from rich.console import Console\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from ariel_experiments.characterize.canonical.core.toolkit import (\n",
    "    CanonicalToolKit as ctk,\n",
    ")\n",
    "\n",
    "console = Console()\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "ARCHIVE_PATH = \"./__data__/archive/features\"\n",
    "K_NEIGHBORS = 15  # Same K you used for fitness\n",
    "\n",
    "# 1. Load Data\n",
    "try:\n",
    "    F_total, generation_ids = load_archive_and_generate_ids(ARCHIVE_PATH)\n",
    "except Exception:\n",
    "    sys.exit()\n",
    "\n",
    "# 2. Run UMAP (The Final Transformation)\n",
    "# We fit on the ENTIRE history to get the global coordinate system\n",
    "\n",
    "# Important: Use the same metric you used during evolution (usually cosine)\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_components=10,\n",
    "    metric=\"cosine\",\n",
    "    n_neighbors=2,\n",
    "    min_dist=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Get the 2D coordinates\n",
    "all_umap_coordinates = umap_reducer.fit_transform(F_total)\n",
    "\n",
    "# 3. Plot\n",
    "\n",
    "# (Assuming 'plot_archive_time_lapse' is defined in your scope)\n",
    "plot_archive_time_lapse(\n",
    "    umap_coordinates=all_umap_coordinates,\n",
    "    generation_ids=generation_ids,\n",
    "    images=None,  # Set to None for pure data visualization\n",
    "    title=f\"Evolutionary Trajectory (Gen 0 - {max(generation_ids)})\",\n",
    "    alpha=0.6,   # Lower alpha helps see clusters\n",
    "    point_size=4,  # Smaller points for large archives\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e0034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe87c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_archive_time_lapse(\n",
    "      umap_coordinates=all_umap_coordinates,\n",
    "      generation_ids=generation_ids,\n",
    "  )\n",
    "\n",
    "# # 2. Plot specific generations (list):\n",
    "# # Plot only generations 0, 10, 20, 30, 40, 49\n",
    "# plot_archive_time_lapse(\n",
    "#     umap_coordinates=all_umap_coordinates,\n",
    "#     generation_ids=generation_ids,\n",
    "#     selected_generations=[0, 10, 20, 30, 40, 49],\n",
    "#     title=\"Selected Generations (0, 10, 20, 30, 40, 49)\",\n",
    "# )\n",
    "\n",
    "# # 3. Plot a range of generations:\n",
    "# # Plot first 10 generations\n",
    "# plot_archive_time_lapse(\n",
    "#     umap_coordinates=all_umap_coordinates,\n",
    "#     generation_ids=generation_ids,\n",
    "#     selected_generations=range(10),\n",
    "#     title=\"First 10 Generations\",\n",
    "# )\n",
    "\n",
    "# # Plot generations 40-49\n",
    "# plot_archive_time_lapse(\n",
    "#     umap_coordinates=all_umap_coordinates,\n",
    "#     generation_ids=generation_ids,\n",
    "#     selected_generations=range(40, 50),\n",
    "#     title=\"Final 10 Generations (40-49)\",\n",
    "# )\n",
    "\n",
    "# # 4. Compare early vs late:\n",
    "# # Plot every 10th generation\n",
    "# plot_archive_time_lapse(\n",
    "#     umap_coordinates=all_umap_coordinates,\n",
    "#     generation_ids=generation_ids,\n",
    "#     selected_generations=range(0, 50, 10),  # 0, 10, 20, 30, 40\n",
    "#     title=\"Every 10th Generation\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef8cd39",
   "metadata": {},
   "source": [
    "imagine tring to get a 2d representtation of the fitness landscape?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ariel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
